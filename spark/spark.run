#!/bin/bash

###############################
#SBATCH --job-name=spark
#SBATCH --output=spark.out
#SBATCH --error=spark.err
###SBATCH --nodelist=fat39
#SBATCH --ntasks=2
#SBATCH --cpus-per-task=80
#SBATCH --ntasks-per-node=1
#SBATCH --time=00:30:00
#SBATCH --account=testproj
#SBATCH --partition=taskp
###############################

module load java/1.7.0
module load python/2.7.10
module load hadoop/2.7.2
module load hadoop/spark2.0.2 

export SPARK_CONF_DIR=conf
source $SPARK_CONF_DIR/spark-env.sh
export PYSPARK_PYTHON=/apps/applications/python/2.7.10/bin/python

export PATH=/apps/applications/python/2.7.10/bin:$PATH
export LD_LIBRARY_PATH=/apps/applications/python/2.7.10/lib:$LD_LIBRARY_PATH

sed -i "s|.*export JAVA_HOME=.*|export JAVA_HOME=${JAVA_HOME}|g" $SPARK_CONF_DIR/spark-env.sh

#start-all.sh ### BUG not working ssh wont pass PATH, LD_... env 

MASTER=$(scontrol show hostname $SLURM_NODELIST | head -n 1)-ib
MASTER_NODE=spark://$MASTER:$SPARK_MASTER_PORT
export SPARK_MASTER_HOST=$MASTER
export SPARK_LOCAL_IP=$MASTER

echo $SPARK_MASTER_HOST
start-master.sh -h $MASTER
srun start-slave.sh $MASTER_NODE

scontrol show hostname $SLURM_NODELIST > ${SPARK_CONF_DIR}/slaves
sed -i 's/$/-ib/' ${SPARK_CONF_DIR}/slaves


#echo "Java:"
#spark-submit \
#        --class org.apache.spark.examples.SparkPi \
#        --master $MASTER_NODE \
#        --total-executor-cores 80 \
#        $SPARK_HOME/examples/jars/spark-examples_2.11-2.0.0.jar \
#        80 

echo "Python:"
spark-submit \
        --master $MASTER_NODE \
        $SPARK_HOME/examples/src/main/python/pi.py \
        160

stop-all.sh
